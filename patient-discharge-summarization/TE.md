# Testing and Evaluation (T&E) Framework for Note Summarization - Patient Discharge Summary

*(use as guidance when completing the CHAI Applied Model Card for a note summarization - patient discharge summary use case)*

## Pre-Deployment

*(additional detail for pre-deployment stages of the AI lifecycle, can be found in the CHAI RAIG)*

### Usefulness, Usability, and Efficacy

*(additional detail for the Responsible AI Principle of Usefulness, Usability, and Efficacy can be found in the CHAI RAIG)*

**Method/Metric:**

* **Physician Documentation Quality Instrument, Nine-item tool (PDQI-9)**
  * Intended Use: 9 part questionnaire that surveys aspects of usefulness. 
  * Rationale: 
  * Reference: [Physician Documentation Quality Instrument (PDQI-9)](https://pmc.ncbi.nlm.nih.gov/articles/instance/3633322/bin/ACI-03-0164-s001.pdf)
  * Open-source tooling:
  
* **DocLens**
  * Intended Use: Assesses coverage of known assertions, or facts, represented in human expert "ground truth" summaries. (1) Extract important facts from ground-truth summaries, either manually or using the LLM-as-a-judge approach (2) extract important facts from AI-generated summaries, either manually or using the LLM-as-a-judge approach (3) calculate accuracy, sensitivity, specificity, PPV, NPV of AI-extracted facts as compared to ground-truth facts.
  * Rationale: Recommend use of LLM-as-judge ways of generating claims from reference summaries and comparing those claims against the LLM-generated summary. One such implementation is DocLens, which captures completeness (i.e., claim recall -- requires reference), conciseness (i.e., claim precision -- requires reference), and attribution accuracy (reference-free). Other implementations include SummaQA.
  * Reference: [DocLens: Multi-aspect Fine-grained Medical Text Evaluation](https://aclanthology.org/2024.acl-long.39/)
  * Open-source tooling:

 * **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**
   * Intended Use: N-gram or Longest Common Subsequence overlap can be measured when reference summary is available.
   * Rationale: Acknowledge but do not use. Modern summarization systems use abstractive summarization (and not extractive summarization) so ROUGE is not helpful.
   * Reference: [ROUGE: A Package for Automatic Evaluation of Summaries](https://aclanthology.org/W04-1013.pdf)
   * Open-source tooling:

 * **BERTScore**
  * Intended Use: BertScore computes similarity scores by aligning generated and reference summaries on a token-level. Token alignments are computed greedily to maximize the cosine similarity between contextualized token embeddings from BERT. Precision, recall, and F1 measure using BERT-based text embeddings
  * Rationale: Acknowledge but optional because hard to interpret for a single summary, may be useful to compare LLMs. Modern summarization systems use abstractive summarization -- token-level comparisons, even relying on embeddings, is too coarse.
  * Reference: [BERTScore: Evaluating Text Generation with BERT](https://arxiv.org/abs/1904.09675)
  * Open-source tooling: [here](https://github.com/Tiiiger/bert_score#readme)

 * **MoverScore**
  * Intended Use: MoverScore (Zhao et al., 2019) measures the semantic distance between a summary and reference text by making use of the Word Mover’s Distance (Kusner et al., 2015) operating over n-gram embeddings pooled from BERT representations.
  * Rationale: Acknowledge but do not use. Modern summarization systems use abstractive summarization -- token-level comparisons, even relying on embeddings, is too coarse.
  * Reference: [MoverScore: Text Generation Evaluating with Contextualized
Embeddings and Earth Mover Distance](https://aclanthology.org/D19-1053.pdf)
  * Open-source tooling:

 * **Sentence Mover's Similarity**
  * Intended Use: Sentence Mover’s Similarity (SMS) (Clark et al., 2019) extends Word Mover’s Distance to view documents as a bag of sentence embeddings as well as a variation which represents documents as both a bag of sentences and a bag of words.
  * Rationale: Acknowledge but do not use. Modern summarization systems use abstractive summarization -- token-level comparisons, even relying on embeddings, is too coarse.
  * Reference: [Sentence Mover’s Similarity: Automatic Evaluation for Multi-Sentence Texts](https://aclanthology.org/P19-1264/)
  * Open-source tooling:

 * **SummaQA**
  * Intended Use: SummaQA (Scialom et al., 2019) applies a BERT-based question-answering model to answer cloze-style questions using generated summaries. Questions are generated by masking named entities in source documents associated with evaluated summaries. The metric reports both the F1 overlap score and QA-model confidence.
  * Rationale: Use DocLens instead
  * Reference: [Answers Unite! Unsupervised Metrics for Reinforced Summarization Models](https://aclanthology.org/D19-1320/)
  * Open-source tooling:

 * **BLANC**
  * Intended Use: BLANC (Vasilyev et al., 2020) is a reference-less metric that measures the performance gains of a pre-trained language model given access to a document summary while carrying out language understanding tasks on the source document’s text.
  * Rationale: Use DocLens instead
  * Reference: [Fill in the BLANC: Human-free quality estimation of document summaries](https://aclanthology.org/2020.eval4nlp-1.2/)
  * Open-source tooling:

 * **SUPERT**
  * Intended Use: SUPERT (Gao et al., 2020) is a reference-less metric, originally designed for multi-document summarization, which measures the semantic similarity of model outputs with pseudo-reference summaries created by extracting salient sentences from the source documents, using soft token alignment techniques.
  * Rationale: Acknowledge but do not use. Modern summarization systems use abstractive summarization -- token-level comparisons, even relying on embeddings, is too coarse.
  * Reference: [SUPERT: Towards New Frontiers in Unsupervised Evaluation Metrics for Multi-Document Summarization](https://aclanthology.org/2020.acl-main.124/)
  * Open-source tooling:

 * **BARTScore**
  * Intended Use: BARTScore is an evaluation metric for assessing the quality of text generated by natural language processing (NLP) models. Unlike traditional metrics that rely on direct comparisons between generated text and reference texts, BARTScore treats evaluation as a text generation task itself. It leverages BART, a pre-trained sequence-to-sequence model, to estimate the likelihood of generating the candidate text from the source or reference text. Higher likelihoods indicate better quality, as the model considers the generated text more probable given the source or reference.
  * Rationale: Acknowledge but optional.
  * Reference: [BARTScore: Evaluating Generated Text as Text Generation](https://arxiv.org/abs/2106.11520)
  * Open-source tooling:

 * **ACUEval**
  * Intended Use: Another LLM breaks down document to atomic content units (Liu et al., 2023b, ACUs), facts that can be verified and cannot be broken down further. ACUEVAL first generates these atomic facts from the system summary, and then validates each extracted fact against the source
document.
  * Rationale: Use DocLens instea. 
  * Reference: [ACUEVAL: Fine-grained Hallucination Evaluation and Correction for Abstractive Summarization](https://openreview.net/pdf/9e1df04bb2315384aa8dbaf47373b833670ae7ff.pdf)
  * Open-source tooling:

 * **Bilingual Evaluation Understudy (BLEU)**
  * Intended Use: 
  * Rationale: Acknowledge but do not use. Modern summarization systems use abstractive summarization (and not extractive summarization) so BLEU is not helpful.
  * Reference: [BLEU: a Method for Automatic Evaluation of Machine Translation](https://aclanthology.org/P02-1040.pdf)
  * Open-source tooling:

 * **METEOR**
  * Intended Use: Accuracy measure primarily used in machine translation but relevant for summarization, improves over BLEU to account for matching synonyms/stemming. Accuracy score similar to but improving on BLEU. 
  * Rationale: Acknowledge but do not use. Modern summarization systems use abstractive summarization (and not extractive summarization) so METEOR is not helpful.
  * Reference: [METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments](https://aclanthology.org/W05-0909.pdf)
  * Open-source tooling:

### Fairness, Equity, and Bias Management

*(additional detail for the Responsible AI Principle of Fairness, Equity, and Bias Management can be found in the CHAI RAIG)*

**Method/Metric:**

 * **here**
  * Intended Use:
  * Rationale: 
  * Reference: []()
  * Open-source tooling:

### Safety and Reliability

*(additional detail for the Responsible AI Principle of Safety and Reliability can be found in the CHAI RAIG)*

Method:

Metric:

### Transparency, Intelligibility, and Accountability

*(additional detail for the Responsible AI Principle of Transparency, Intelligibility, and Accountability can be found in the CHAI RAIG)*

Method:

Metric:

### Security and Privacy

*(additional detail for the Responsible AI Principle of Security and Privacy can be found in the CHAI RAIG)*

Method:

Metric:

## Post-Deployment

*(additional detail for post-deployment stages of the AI lifecycle, can be found in the CHAI RAIG)*

### Usefulness, Usability, and Efficacy

*(additional detail for the Responsible AI Principle of Usefulness, Usability, and Efficacy can be found in the CHAI RAIG)*

Method:

Metric:

### Fairness, Equity, and Bias Management

*(additional detail for the Responsible AI Principle of Fairness, Equity, and Bias Management can be found in the CHAI RAIG)*

Method:

Metric:

### Safety and Reliability

*(additional detail for the Responsible AI Principle of Safety and Reliability can be found in the CHAI RAIG)*

Method:

Metric:

### Transparency, Intelligibility, and Accountability

*(additional detail for the Responsible AI Principle of Transparency, Intelligibility, and Accountability can be found in the CHAI RAIG)*

Method:

Metric:

### Security and Privacy

*(additional detail for the Responsible AI Principle of Security and Privacy can be found in the CHAI RAIG)*

Method:

Metric:
